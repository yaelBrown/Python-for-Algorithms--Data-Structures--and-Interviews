o(1) = constant time

Using the right data structure is important

Abstract Data Structures - aDS are the specifications
  stack - (arr, ll)
  queue - (arr, ll)
  priority queue - (heap)
  dictionary/hashmap - (arr)

Data Structures - DS are the concrete implementations
  Arrays
  linked lists
  heap

  using ds in main can reduce the running time of applications


Arrays
  o(1) - time complexiy
  Are random access
  Uses less memory
  to remove a item you have re-adjust the array. 
    o(1) + o(n)
  do not need extra memory (because you dont have to store pointers)
  insert at the beginning = o(n) runtime


LL 
  o(n) - time complexity
  are not random access
  Are dynamic
  Use pointers for next item, and previous item
  Is very fast, adding items is linear time
    insert items at the beginning
  To add to the end of the ll you have to traverse to the end of the lists
    this runs at linear time depending on size of ll O(n)

  Low level memory management
    when dealing with c - malloc() and free()
    can manipulate the heap memory

    syslinux.org q?heap management

  alt-tab in windows
  photoviewer with next image, and previous image
  blockchain - where blocks are linked cryptographically linked together

  most popular data structure
  searching is sequential


dLL
  has a reference to the next node and a reference to the previous node


stack 
  aDS - usually represented with interface (in java)
  basic operations: pop(), push(), and peek()
  LIFO - last in, first out "lee-foo"
  used with graph algorithms: depth-first search (dfs) /or recursion
  finding Euler-cycles in a graph
  finding strongly connected components in a graph

  stack recursion - stacks that are created recursively
    forget the base case you will cause stack overflow

  applications
    back button in web browsers
    edit or undo
    stack memory stores

  stack memory - most important implementation using the stack datatype
    call stack is a aDS stores information about the active subroutines/methods/functions of a computer program
    details are normally hidden in high-level programming languages
    keeps track of the point to which each active subroutine should return control when it finishes executing
    stores temp vars created by each func
    func creates a new var its pushed onto the stack
    stack memory is limited
    local variables are removed when the item on the stack is removed. 
    c and c++ stack memory has to be managed


heap
  heap memory - region of memory that is not managed automatically for you
    large region of memory
    c: malloc() calloc() with pointers
    java: reference types and objects are on the heap
    we have to deallocate these memory chunks because they are not managed automatically
      if not will cause a memory leak
      thats why java garbage collection came to be
    runs slower because of pointers


queue
  aDS (in java its a interface)
  basic ops: enqueue() and dequeue(), peek()
  FIFO structure: first in first out
  impl dynamic array or linked lists
  important when impl bfs for graphs
  enqueue adds a item to the first of the queue
  dequeue remove the first item in the queue

  applications
    in serveral graph algorithms
    cpu scheduling
    data is transferred asynchronously: io buffers
    android os uses queues a lot
    operational research applcations or stochastic models rely heavily on queue
    stacks and queues are fundamental building blocks for operating systems

  better solution would be to use a dLL


Binary Search Trees (bst)
  bst are data structures
  bst makes operations fast
    o(log N) ~ predictable
    get rid of half the day for every search
  nodes can only have 2 children
    left child and right child
  arrays have to be sorted to be searched
  height - number of layers it contains
  height of the tree at the minimum
    h=log n
  balanced - when the tree has equal amount of verticies on left and right
    have better runtime
  
  Trees - have nodes (vertex) with the data and connection between the nodes (edges)
    child - node connected to another node (one level down)
    leaf nodes - nodes with no children

  Consider nodes, if smaller goto left, if bigger go right

  logarithmic time complexity for searching. 
  logarithmic time for insertion if the tree is balanced. 
    if unbalanced it is linear time

  deletiong - (soft) - don't remove the node, just mark that its been removed
    not so efficient solution

    search, then remove
    when you remove a node, remove the pointer from the parent
      O(logN) + O(1)

    when removing root node you have to find the predecessor in left subtree
      predecessor - highest node in the left subtree
      successor - lowest node in the right subtree

      swap the root node with predecessor (or the node with no children out of predecessor or successor)

      O(logN)

  Traversal 
    in-order traversal - visit the left subtree  + root + right subtree recursively (in order)
    pre-order traversal - root + left subtree + right subtree
    post-order traversal - left subtree + right subtree + root
 
  Running times
    Space 
      avg - O(n)
      wst - o(n)
    Insert, Delete, Search
      avg - o(log n)
      wst - o(n)

  Implementing
    Node class {data, leftChild, rightChild, parent}

  Applications
    Operating systems
      directories are represented in trees
      windows: 'tree' in cmd prompt
    game trees (chess and tic-tac-toe)
    machine learning - decision trees and boosting


Balanced Binary Trees (AVL)
  Running time of bst depends on the height of the binary search tree. 
  1962 invented by two russian computer scientist
  avl trees are faster than red-black trees because thy are more rigidly balanced but needs more work
    AVL trees are better for search operations
  os's rely heavily on these data structures
  operations are the same
  every insertion have to check if tree is balanced or not
  delete/max/min operations are the same

  AVL tree or Red-Black tree
    o(logN) is guaranteed

  Construct a binary tree from a sorted array, ends up being a linked list
    o(logN) reduced to o(n)

  rotating a tree - is the action of making a tree balanced 
    [impl is confusing]

    doublely left heavy/right heavy

    To rotate right - just update the references
      have to create a tempNode

    if difference between 2 sub-nodes their value (when counting height) does not exceed 1, they are considered balanced. 
      [see photo avl-balanced-subtree.png]

  Insert - o(n*logN)
  In-Order traversal - o(n)
  Overall complexity - o(n*logN)

  QuickSort out performs AVL sort because QuickSort does not use additional memory. 
    Running time will be the same for all of them.

  Applications
    For a lookup intensive task, use a AVL tree
    Insertion/Delection is not as fast - Because we have to keep rebalancing the tree
    Databases - when deletions or insertions are not so frequent, but have to make a lot of look-ups
    Look-up Tables - impl w/hashtables, AVL trees support more ops in the main
    Can sort with help of AVL trees
    Red-Black trees are a bit more popular - AVL trees require more rotations (slower)

  Impl
    You have deal with height parameter. 
    Leaf nodes height is 0 (their left or right node are -1) 
      Going up to root they increase by 1

    Have to update the childs, parents, and heights (in that order)


Red-Black Trees
  Solves same problem as AVL trees
  Running time of BST depends on the height of BST. 
  AVL trees are faster than red-black trees, because they are rigidly balanced
  OS require heavily on these data structures
  RBT are not rigidly balanced, so they dont have to be rebalanced all of the time

  Each node is either red or black
    Root is always black
    child missing = NILL or NULL (and they are block)

    Every red node must have 2 black children (NIL counts as children)
      Also a black parent

  Rotations are exactly the same
  Every new node will be Red by default
  Violate the RB properties we have to rebalance the tree:
    Make rotations
    Update the colors

  Purpose of coloring the nodes is to ensure that you end up with a balanced tree. 
    No path is more than twice as long as any other path in the tree
    tree is aproximately balanced

  Rotations
    Root node can be red. Red nodes have to have 2 black children. 
      have to check recursively if red-black properties are met or not
    Rotate first, then recolor second. 

    After rotating you have to check and make sure you are not violating the red-black properties from other parts of the tree

  Applications
    Linux kernels relies heavily on red-black trees data structure
    For a insert intensive task, use a red-black tree
    Java: java.util.TreeMap, java.util.TreeSet
    Cpp STL: map, multimap, multiset 

  Impl
    Need to create a class for the color - in the example 'RED = 1; BLACK = 2'
    Constructor default for node is Color=Color.RED
    Need a isViolated or violate function
      Used to start recolor or rotate nodes


Priority queues
  Abstract datatype - typically created with 1 dimensional array
  Priorities are usually impl with heaps or self-balancing trees
  No FIFO structure

  Has a priority value

  concept naturally suggest a sorting algorithm

  Heap Basics
    Basically a binary tree.
      abstract data type - because underlying datastructure could be an 1 dimensional array
    2 heap types min and max heap
    
    dijkstras algo, prims algo

    Priority queues are impl w/heaps 

    heap properties are complete. 
      construct heap from left to right across each row
    binary heap every node can have 2 children, lc rc
      does not matter if lc is less than or greater than rc (different from bst's)
      order does not matter

    min heap - parent is always smaller than the values of the children
    max heap - parent is always greater

    root - will be the smallest/greatest value in the heap

    1d arr representation
      i = parent node
      2i+1 = left node
      2i+2 = right node

    heapify process
      o(n) to construct the heap
      o(logN) + o(n) + o(logN) = o(N) - reconstruct the heap if properties are isViolated
      inserting item to heap is just adding data to the array and incrementing the index

      violated = when one of the child nodes are greater than root in max heap or smaller than root in min heap

    deleting item o(n) + o(logN) = o(n)
      has ti search and find the item, then reconstruct the ds

  HeapSort
    comparison-based sorting algorithm
    uses heap data structure
    in place algo, does not need additional memory
    running time to sort o(n*logN) 
      Similar running time for merge sort/quick sort

  Binomial Heap
    similar to binary heap but supports quick merging of two heaps
    impl of mergeable heap aDS (meldable heap)
    priority queue + supporting merge operational
    impl as collection of tree
    insertion o(logN) can be reduced to o(1)

  Fibonacci Heap 
    Faster than classic binary heap
    Dijkstra sortest path algo, and prims spanning tree algo run faster if they rely on fib. heap 
    can have serveral children
      number of children are kept low
    can achieve o(1) for operations instead of o(logN)
































#69




