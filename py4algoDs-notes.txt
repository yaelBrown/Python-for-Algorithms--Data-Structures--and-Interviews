o(1) = constant time

Using the right data structure is important

Abstract Data Structures - aDS are the specifications
  stack - (arr, ll)
  queue - (arr, ll)
  priority queue - (heap)
  dictionary/hashmap - (arr)

Data Structures - DS are the concrete implementations
  Arrays
  linked lists
  heap

  using ds in main can reduce the running time of applications


Arrays
  o(1) - time complexiy
  Are random access
  Uses less memory
  to remove a item you have re-adjust the array. 
    o(1) + o(n)
  do not need extra memory (because you dont have to store pointers)
  insert at the beginning = o(n) runtime


LL 
  o(n) - time complexity
  are not random access
  Are dynamic
  Use pointers for next item, and previous item
  Is very fast, adding items is linear time
    insert items at the beginning
  To add to the end of the ll you have to traverse to the end of the lists
    this runs at linear time depending on size of ll O(n)

  Low level memory management
    when dealing with c - malloc() and free()
    can manipulate the heap memory

    syslinux.org q?heap management

  alt-tab in windows
  photoviewer with next image, and previous image
  blockchain - where blocks are linked cryptographically linked together

  most popular data structure
  searching is sequential


dLL
  has a reference to the next node and a reference to the previous node


stack 
  aDS - usually represented with interface (in java)
  basic operations: pop(), push(), and peek()
  LIFO - last in, first out "lee-foo"
  used with graph algorithms: depth-first search (dfs) /or recursion
  finding Euler-cycles in a graph
  finding strongly connected components in a graph

  stack recursion - stacks that are created recursively
    forget the base case you will cause stack overflow

  applications
    back button in web browsers
    edit or undo
    stack memory stores

  stack memory - most important implementation using the stack datatype
    call stack is a aDS stores information about the active subroutines/methods/functions of a computer program
    details are normally hidden in high-level programming languages
    keeps track of the point to which each active subroutine should return control when it finishes executing
    stores temp vars created by each func
    func creates a new var its pushed onto the stack
    stack memory is limited
    local variables are removed when the item on the stack is removed. 
    c and c++ stack memory has to be managed


heap
  heap memory - region of memory that is not managed automatically for you
    large region of memory
    c: malloc() calloc() with pointers
    java: reference types and objects are on the heap
    we have to deallocate these memory chunks because they are not managed automatically
      if not will cause a memory leak
      thats why java garbage collection came to be
    runs slower because of pointers


queue
  aDS (in java its a interface)
  basic ops: enqueue() and dequeue(), peek()
  FIFO structure: first in first out
  impl dynamic array or linked lists
  important when impl bfs for graphs
  enqueue adds a item to the first of the queue
  dequeue remove the first item in the queue

  applications
    in serveral graph algorithms
    cpu scheduling
    data is transferred asynchronously: io buffers
    android os uses queues a lot
    operational research applcations or stochastic models rely heavily on queue
    stacks and queues are fundamental building blocks for operating systems

  better solution would be to use a dLL


Binary Search Trees (bst)
  bst are data structures
  bst makes operations fast
    o(log N) ~ predictable
    get rid of half the day for every search
  nodes can only have 2 children
    left child and right child
  arrays have to be sorted to be searched
  height - number of layers it contains
  height of the tree at the minimum
    h=log n
  balanced - when the tree has equal amount of verticies on left and right
    have better runtime
  
  Trees - have nodes (vertex) with the data and connection between the nodes (edges)
    child - node connected to another node (one level down)
    leaf nodes - nodes with no children

  Consider nodes, if smaller goto left, if bigger go right

  logarithmic time complexity for searching. 
  logarithmic time for insertion if the tree is balanced. 
    if unbalanced it is linear time

  deletiong - (soft) - don't remove the node, just mark that its been removed
    not so efficient solution

    search, then remove
    when you remove a node, remove the pointer from the parent
      O(logN) + O(1)

    when removing root node you have to find the predecessor in left subtree
      predecessor - highest node in the left subtree
      successor - lowest node in the right subtree

      swap the root node with predecessor (or the node with no children out of predecessor or successor)

      O(logN)

  Traversal 
    in-order traversal - visit the left subtree  + root + right subtree recursively (in order)
    pre-order traversal - root + left subtree + right subtree
    post-order traversal - left subtree + right subtree + root
 
  Running times
    Space 
      avg - O(n)
      wst - o(n)
    Insert, Delete, Search
      avg - o(log n)
      wst - o(n)

  Implementing
    Node class {data, leftChild, rightChild, parent}

  Applications
    Operating systems
      directories are represented in trees
      windows: 'tree' in cmd prompt
    game trees (chess and tic-tac-toe)
    machine learning - decision trees and boosting


Balanced Binary Trees (AVL)
  Running time of bst depends on the height of the binary search tree. 
  1962 invented by two russian computer scientist
  avl trees are faster than red-black trees because thy are more rigidly balanced but needs more work
    AVL trees are better for search operations
  os's rely heavily on these data structures
  operations are the same
  every insertion have to check if tree is balanced or not
  delete/max/min operations are the same

  AVL tree or Red-Black tree
    o(logN) is guaranteed

  Construct a binary tree from a sorted array, ends up being a linked list
    o(logN) reduced to o(n)

  rotating a tree - is the action of making a tree balanced 
    [impl is confusing]

    doublely left heavy/right heavy

    To rotate right - just update the references
      have to create a tempNode

    if difference between 2 sub-nodes their value (when counting height) does not exceed 1, they are considered balanced. 
      [see photo avl-balanced-subtree.png]

  Insert - o(n*logN)
  In-Order traversal - o(n)
  Overall complexity - o(n*logN)

  QuickSort out performs AVL sort because QuickSort does not use additional memory. 
    Running time will be the same for all of them.

  Applications
    For a lookup intensive task, use a AVL tree
    Insertion/Delection is not as fast - Because we have to keep rebalancing the tree
    Databases - when deletions or insertions are not so frequent, but have to make a lot of look-ups
    Look-up Tables - impl w/hashtables, AVL trees support more ops in the main
    Can sort with help of AVL trees
    Red-Black trees are a bit more popular - AVL trees require more rotations (slower)

  Impl
    You have deal with height parameter. 
    Leaf nodes height is 0 (their left or right node are -1) 
      Going up to root they increase by 1

    Have to update the childs, parents, and heights (in that order)


Red-Black Trees
  Solves same problem as AVL trees
  Running time of BST depends on the height of BST. 
  AVL trees are faster than red-black trees, because they are rigidly balanced
  OS require heavily on these data structures
  RBT are not rigidly balanced, so they dont have to be rebalanced all of the time

  Each node is either red or black
    Root is always black
    child missing = NILL or NULL (and they are block)

    Every red node must have 2 black children (NIL counts as children)
      Also a black parent

  Rotations are exactly the same
  Every new node will be Red by default
  Violate the RB properties we have to rebalance the tree:
    Make rotations
    Update the colors

  Purpose of coloring the nodes is to ensure that you end up with a balanced tree. 
    No path is more than twice as long as any other path in the tree
    tree is aproximately balanced

  Rotations
    Root node can be red. Red nodes have to have 2 black children. 
      have to check recursively if red-black properties are met or not
    Rotate first, then recolor second. 

    After rotating you have to check and make sure you are not violating the red-black properties from other parts of the tree

  Applications
    Linux kernels relies heavily on red-black trees data structure
    For a insert intensive task, use a red-black tree
    Java: java.util.TreeMap, java.util.TreeSet
    Cpp STL: map, multimap, multiset 

  Impl
    Need to create a class for the color - in the example 'RED = 1; BLACK = 2'
    Constructor default for node is Color=Color.RED
    Need a isViolated or violate function
      Used to start recolor or rotate nodes


Priority queues
  Abstract datatype - typically created with 1 dimensional array
  Priorities are usually impl with heaps or self-balancing trees
  No FIFO structure

  Has a priority value

  concept naturally suggest a sorting algorithm

  Heap Basics
    Basically a binary tree.
      abstract data type - because underlying datastructure could be an 1 dimensional array
    2 heap types min and max heap
    use heaps when you want smallest (or max) at o(1) complexity because it is always the root node
    
    dijkstras algo, prims algo

    Priority queues are impl w/heaps 

    heap properties are complete. 
      construct heap from left to right across each row
    binary heap every node can have 2 children, lc rc
      does not matter if lc is less than or greater than rc (different from bst's)
      order does not matter

    min heap - parent is always smaller than the values of the children
    max heap - parent is always greater

    root - will be the smallest/greatest value in the heap

    1d arr representation
      i = parent node
      2i+1 = left node
      2i+2 = right node

    heapify process
      o(n) to construct the heap
      o(logN) + o(n) + o(logN) = o(N) - reconstruct the heap if properties are isViolated
      inserting item to heap is just adding data to the array and incrementing the index

      violated = when one of the child nodes are greater than root in max heap or smaller than root in min heap

      from heapq import heappop, heappush, heapify
        package that contains heap operations

    deleting item o(n) + o(logN) = o(n)
      has ti search and find the item, then reconstruct the ds

  HeapSort
    comparison-based sorting algorithm
    uses heap data structure
    in place algo, does not need additional memory
    running time to sort o(n*logN) 
      Similar running time for merge sort/quick sort

  Binomial Heap
    similar to binary heap but supports quick merging of two heaps
    impl of mergeable heap aDS (meldable heap)
    priority queue + supporting merge operational
    impl as collection of tree
    insertion o(logN) can be reduced to o(1)

  Fibonacci Heap 
    Faster than classic binary heap
    Dijkstra sortest path algo, and prims spanning tree algo run faster if they rely on fib. heap 
    can have serveral children
      number of children are kept low
    can achieve o(1) for operations instead of o(logN)


Associative Arrays (has maps, maps)
  Associative arrays/maps/dictionaries are aDS
  k/v pairs, where each key appears once. 
  aim is to reach o(1) time complexity for most of the operations
  impl w/ hashtables but binary search trees can be used

  supports add/remove/update/lookup operations

  does not support sorting operations
  no order defined in these k/v pairs

  HashTables/dictionaries
    Collisions are when keys are mapped to the same index within a bucket. 
      open addressing (better solution) - creates a new space in bucket for the key
      chaining - multiple entries into the same slot with the help of a linked list

      linear probing - try next slot if collision appears
      quadratic probing - try slots 1,2,4,8,... units far always
      rehashing - hash the result again in order to find an empty slot

      finding space is o(n)

      Underlying datastructure for ht/dict is Arrays

      They do not support sorting

    Dynamic Resizing 
      performance depends on load factor: number entries to number of buckets ratio. 
      space-time tradeoff - resize the table when its load factor exceeds given threshold

      Java - load factor is greater than 0.75 hashmap will automatically be resized
      Python - load factor is greater than 0.66 (2/3) it will automatically be resized. 
        Resizing takes o(n) time to complete
        may make dynamic-sized hash tables inappropriate for real-time applications

    Appl
      Databases (sometimes search trees, hashing is better)
      counting given word occurence in a particular document
      storing data + lookup tables (password checks)
      lookup tables in huge networks (lookup for ip addresses)
        (routing tables?)
      Rabin-karp algo for substring search

    Why use prime numbers in hashing? - http://www.globalsoftwaresupport.com/use-prime-numbers-hash-functions/

    probing is when you try to create a key in the batch array slot. 
      hashfunction() - creates a random number for index in batch array to store key. Then eventually store the value

    Hashing Appl
      saving password string into Databases
      blockchain sha-256 hashes - hashes identify the block.
        blockchains are linkedlist
        if one block is changed the hash is changed. 
      bitcoin

    
Ternary Search Trees / tries
  tries allow you search sort things efficiently
  consume a lot of memory
  better to use tst - which stores less references and null objects
  tst - stores characters or strings in nodes
  each node has 3 children
    left - less
    middle - equal
    right - greater
    tries can have as many children as the alphabet
  can balance tsts-s with rotations but not worth it
  can be used instead of hashmap but not as efficient as hashing
    hashing needs to examine the entire key, TST does not

  tst is better than hashing, and supports sorting

  put() - insertion method
  tst is better than hashtables
    lzw data compression
  tst are more efficient and memory friendly because it does not have as many children as tries

  can combime tst with t (tst+t)

  tst works only for strings
  only examines just enough key characters 
  search miss may only involve a few characters
  support more ops (sorting)
  faster than hashing and more flexible than bst

  hashing needs to examine entire key
  search hits and misses cost the same
  rt and perf relies heavily on the hashfunction
  does not support as many ops as tst (hashing)

  Appl
    auto-complete feature
    used for spell checkers
    near neighbor searching
    dbs especially when indexing by serveral non-key fields is desirable
    package routing
    prefix matching - google search
      google relies heavily on tst
      can use dfs (depth-first search)


Graph Theory
  verticies == nodes
  edges == paths 

  undirection graphs (ug) - edges are bidirectional
    edges may have weights
  directed graphs (dg) - edges have direction
    edges may have weights as well. 

  tree - ug - any two verticies are connected by exactly one path
  forest - dg - disjoint union of trees. (in one graph the graph has multiple graphs)
  directed acyclic graph - dag - finite directed graph with no directed cycles
    with no directed cycles
    crucial in many algorithms

  complete graph - when every pair of verticies are connected

  How to represent
    adjacency list - an array that has all the connections and routes of that node
      good - iterating over all the edges is efficient
      bad - edge weight lookup is slow because it is in o(n) (linear time)
    adjacency matrix - a matrix (2d array) that holds the edge weights/paths
      bad - as many rows and columns as verticies in the graph
        iterating over all the edges takes o(e^2)
      good - space efficient with dense graphs but requires o(v^2) memory
        edge weight lookup is o(1)

      undirected graphs have symmetric adjacency matrix representations

  appl
    sortest path algos (google maps, waze)
      dijkstra shortest path algo - find the shortest path between two locations quite fast
      arbitrage opportunities on the FOREX - dected with help from Bellman-Ford algo, detecting negative cycles
      shortest path algo used in photo editing softwares (photoshop, gimp)
      open shortest path first (OSPF) - routing protocol
      graph traversal algo (breath-first search) can traverse huge networks such as WWW
        googles Spider crawls the web and keeps indexing the web pages (page rank calculation)
      job scheduling algo
      maven or gradle use graph algo to prevent the problem of cycles in a graph and (dags)
      quadratic optimization problems
        maximum flow problem (max flow min cut problem)
        circulation problem and airline scheduling
        vehicle routing problem
      strongly connected components
        recommendation systems (youtube related videos) (youtube currently uses mesh learning)
        analyze ecosystems

Graph Traversal Algo
  BFS is usually implemented with a queue aDS*** interview question "What is main difference between dfs and bfs"
  DFS is usually implemented with a stack. But usually implemented with recursion*** interview question "What is main difference between dfs and bfs"
    call it recurisively the os will use a stack no matter what
  
  breadth-first search (bfs)
    we want to visit every node 
    visit every vertex exactly once 
    visit neighbors then the neighbors of these new verticies
    running time complexity: o(v+e)
    memory complexity is not good - have to store a lot of references
    dfs is usually preferred
    constructs shortest path
      dijkstra algo does bfs if all the edge weights are == 1

    going to visit every node row by row
    not very memory friendly
    have to store a lot in queue datatype

    appl
      ai/ml - robots can discover the surrounding more easily with bfs than dfs
      maximum flow - edmonds-karp algo uses bfs for finding augmenting paths
      garbage collection - cheyens algo, help maintain active references in heap memory
        usues bfs to detect all the references on the heap 
      serialization/deserialization of a tree - (when order does not matter) - allows the tree to be reconstructed in a efficient manner

  Depth-First Search (dfs)
    widely used traversal algoinvest
    investigated as a strategy for solving mazes by Tremaux in 19th century (what?)
    goes deep as possible in each branch before back tracking
    running time complexity: o(v+e)
    memory complexity: little bit better than bfs

    can use recursion or iteration
      have to use stack aDS
      iter/recur are the same

    appl
      topological ordering
      kosaraju algo - finding strongly connecting components
        used in recommendation systems
      detecting cycles - if a graph is DAG or not
      generating or finding a way out of a maze.

  Memory management
    using queue o(n)
    dfs saving memory is o(logN)

    dfs: o(logN)
    bfs: o(n)

    dfs is preferred because of its memory complexity
    bfs is preferred sometimes but for ai/ml algo's

  Shortest Path (sp)
    sp prob - finding a path between two verticies in a graph such that sum of the weights of its edges is minimized
      dijkstra algo
      bellman-ford
      a* search
      floyd-warshall algo
    
    dijkstra algo - O(v*logV+e)
      it is a greedy algo
      fast
      
    
